{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vincent\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\n",
      "Requirement already satisfied: torch in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.16.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy \n",
    "!pip install pandas\n",
    "!pip install torch\n",
    "!pip install sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating data\n",
    "* Import csv we want\n",
    "* Map our features to usuable integers\n",
    "* Map our labels to a one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       SIFT_pred  LRT_pred  MutationTaster_pred  FATHMM_pred  clinvar_clnsig\n",
      "12223         -1         1                   -1            1               0\n",
      "12226          0         0                   -1            0               0\n",
      "12227          1         0                    1           -1               1\n",
      "12229         -1         0                   -1           -1               0\n",
      "12236         -1         0                   -1           -1               0\n",
      "(4535, 5) (1512, 5)\n"
     ]
    }
   ],
   "source": [
    "# FIXME: Give a proper path/subset of data\n",
    "df = pd.read_csv('C:/Projects/ClinBall/ignored_files/out1.tsv', sep='\\t')\n",
    "\n",
    "# The more negative the score, the more confident the predictor is about the damaging prediction\n",
    "# The more positive the score, the more confident the predictor is about the non-damaging prediction\n",
    "# A score of zero is unknown.\n",
    "features_mapping = {\n",
    "    'SIFT_pred': {\n",
    "        'T': 1,\n",
    "        'D': -1,\n",
    "    },\n",
    "    'LRT_pred': {\n",
    "        'N': 1,\n",
    "        'D': -1,\n",
    "        'U': 0,\n",
    "    },\n",
    "    'MutationTaster_pred': {\n",
    "        'N': 1,\n",
    "        'D': -1,\n",
    "        'A': -2,\n",
    "        'P': 2,\n",
    "    },\n",
    "    'FATHMM_pred': {\n",
    "        'T': 1,\n",
    "        'D': -1,\n",
    "    }\n",
    "}\n",
    "\n",
    "clinvar_map = {\n",
    "    'clinvar_clnsig': {\n",
    "        'Pathogenic': 0,\n",
    "        'Likely_pathogenic': 0,\n",
    "        'Pathogenic/Likely_pathogenic': 0,\n",
    "        'Benign/Likely_benign': 1,\n",
    "        'Likely_benign': 1,\n",
    "        'Benign': 1\n",
    "    }\n",
    "}\n",
    "\n",
    "# In dbNSFP, entries with no value have a period, change this for a 0 to match our schema\n",
    "df = df.replace({'.': 0})\n",
    "\n",
    "# Apply mappings\n",
    "df = df.replace(features_mapping)\n",
    "df = df.replace(clinvar_map)\n",
    "\n",
    "# Replace all strange/ambiguous ClinVar clnsig with a common integer\n",
    "df = df.replace({'\\D': -1}, regex=True)\n",
    "# Drop all rows which have a strange/ambiguous clinvar clnsig\n",
    "df = df[df.clinvar_clnsig != -1]\n",
    "\n",
    "# Split our data into two separate sets for training purposes\n",
    "train, test = train_test_split(df, shuffle=False)\n",
    "\n",
    "print(test.head())\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "* Implement the Dataset class supplied by PyTorch\n",
    "  * Used Map style\n",
    "  * API: https://pytorch.org/docs/stable/data.html#map-style-datasets\n",
    "* Instanciate DataLoader using Dataset instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetdbNSFP(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data.values.astype('float'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "            target = self.data[index][-1]\n",
    "            data_val = self.data[index] [:-1]\n",
    "            return data_val,target\n",
    "        \n",
    "train_dataset = DataSetdbNSFP(train)\n",
    "valid_dataset = DataSetdbNSFP(train)\n",
    "test_dataset = DataSetdbNSFP(test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=1000, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=1000, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1512, shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "* This model is a simple implementation\n",
    "* Based on https://github.com/ieee8023/NeuralNetwork-Examples/blob/master/pytorch/pytorch-mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # In: nb of features, out: nb of examples\n",
    "        self.fc = nn.Linear(4, 1000)\n",
    "        # In: nb of examples, out: nb of predictions\n",
    "        self.fc2 = nn.Linear(1000, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 4))\n",
    "        h = F.relu(self.fc(x))\n",
    "        h = self.fc2(h)\n",
    "        # Softmax to get the actual labels\n",
    "        return F.softmax(h, dim=1)    \n",
    "    \n",
    "model = Model()\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "\n",
    "# Optimizer based on Adam algorithm, uses a slightly lower rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "* Run a fixed number of epochs\n",
    "* After each epoch, run the validation set to adjust learning\n",
    "* Print results\n",
    "* Based on example: https://github.com/ieee8023/NeuralNetwork-Examples/blob/master/pytorch/pytorch-mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 1/20 [0/4535 (0%)]\tLoss: 0.645696\n",
      " Train Epoch: 1/20 [1000/4535 (20%)]\tLoss: 0.635708\n",
      " Train Epoch: 1/20 [2000/4535 (40%)]\tLoss: 0.633859\n",
      " Train Epoch: 1/20 [3000/4535 (60%)]\tLoss: 0.623889\n",
      " Train Epoch: 1/20 [2140/4535 (80%)]\tLoss: 0.617502\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 1/20 [4535/4535 (80%)]\tLoss: 0.617502\t Validation Accuracy: 86.68%\n",
      " Train Epoch: 2/20 [0/4535 (0%)]\tLoss: 0.608764\n",
      " Train Epoch: 2/20 [1000/4535 (20%)]\tLoss: 0.602112\n",
      " Train Epoch: 2/20 [2000/4535 (40%)]\tLoss: 0.597655\n",
      " Train Epoch: 2/20 [3000/4535 (60%)]\tLoss: 0.589716\n",
      " Train Epoch: 2/20 [2140/4535 (80%)]\tLoss: 0.595296\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 2/20 [4535/4535 (80%)]\tLoss: 0.595296\t Validation Accuracy: 87.30%\n",
      " Train Epoch: 3/20 [0/4535 (0%)]\tLoss: 0.577744\n",
      " Train Epoch: 3/20 [1000/4535 (20%)]\tLoss: 0.576170\n",
      " Train Epoch: 3/20 [2000/4535 (40%)]\tLoss: 0.573524\n",
      " Train Epoch: 3/20 [3000/4535 (60%)]\tLoss: 0.558778\n",
      " Train Epoch: 3/20 [2140/4535 (80%)]\tLoss: 0.566412\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 3/20 [4535/4535 (80%)]\tLoss: 0.566412\t Validation Accuracy: 87.30%\n",
      " Train Epoch: 4/20 [0/4535 (0%)]\tLoss: 0.551617\n",
      " Train Epoch: 4/20 [1000/4535 (20%)]\tLoss: 0.553152\n",
      " Train Epoch: 4/20 [2000/4535 (40%)]\tLoss: 0.551491\n",
      " Train Epoch: 4/20 [3000/4535 (60%)]\tLoss: 0.543517\n",
      " Train Epoch: 4/20 [2140/4535 (80%)]\tLoss: 0.533671\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 4/20 [4535/4535 (80%)]\tLoss: 0.533671\t Validation Accuracy: 87.30%\n",
      " Train Epoch: 5/20 [0/4535 (0%)]\tLoss: 0.535302\n",
      " Train Epoch: 5/20 [1000/4535 (20%)]\tLoss: 0.539656\n",
      " Train Epoch: 5/20 [2000/4535 (40%)]\tLoss: 0.528503\n",
      " Train Epoch: 5/20 [3000/4535 (60%)]\tLoss: 0.523602\n",
      " Train Epoch: 5/20 [2140/4535 (80%)]\tLoss: 0.513819\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 5/20 [4535/4535 (80%)]\tLoss: 0.513819\t Validation Accuracy: 87.45%\n",
      " Train Epoch: 6/20 [0/4535 (0%)]\tLoss: 0.525264\n",
      " Train Epoch: 6/20 [1000/4535 (20%)]\tLoss: 0.517362\n",
      " Train Epoch: 6/20 [2000/4535 (40%)]\tLoss: 0.516714\n",
      " Train Epoch: 6/20 [3000/4535 (60%)]\tLoss: 0.515611\n",
      " Train Epoch: 6/20 [2140/4535 (80%)]\tLoss: 0.487842\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 6/20 [4535/4535 (80%)]\tLoss: 0.487842\t Validation Accuracy: 87.56%\n",
      " Train Epoch: 7/20 [0/4535 (0%)]\tLoss: 0.514040\n",
      " Train Epoch: 7/20 [1000/4535 (20%)]\tLoss: 0.505264\n",
      " Train Epoch: 7/20 [2000/4535 (40%)]\tLoss: 0.502893\n",
      " Train Epoch: 7/20 [3000/4535 (60%)]\tLoss: 0.491372\n",
      " Train Epoch: 7/20 [2140/4535 (80%)]\tLoss: 0.504758\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 7/20 [4535/4535 (80%)]\tLoss: 0.504758\t Validation Accuracy: 87.56%\n",
      " Train Epoch: 8/20 [0/4535 (0%)]\tLoss: 0.491304\n",
      " Train Epoch: 8/20 [1000/4535 (20%)]\tLoss: 0.497845\n",
      " Train Epoch: 8/20 [2000/4535 (40%)]\tLoss: 0.491243\n",
      " Train Epoch: 8/20 [3000/4535 (60%)]\tLoss: 0.498437\n",
      " Train Epoch: 8/20 [2140/4535 (80%)]\tLoss: 0.490948\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 8/20 [4535/4535 (80%)]\tLoss: 0.490948\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 9/20 [0/4535 (0%)]\tLoss: 0.492869\n",
      " Train Epoch: 9/20 [1000/4535 (20%)]\tLoss: 0.488225\n",
      " Train Epoch: 9/20 [2000/4535 (40%)]\tLoss: 0.481266\n",
      " Train Epoch: 9/20 [3000/4535 (60%)]\tLoss: 0.485937\n",
      " Train Epoch: 9/20 [2140/4535 (80%)]\tLoss: 0.484042\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 9/20 [4535/4535 (80%)]\tLoss: 0.484042\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 10/20 [0/4535 (0%)]\tLoss: 0.485508\n",
      " Train Epoch: 10/20 [1000/4535 (20%)]\tLoss: 0.479229\n",
      " Train Epoch: 10/20 [2000/4535 (40%)]\tLoss: 0.474198\n",
      " Train Epoch: 10/20 [3000/4535 (60%)]\tLoss: 0.480809\n",
      " Train Epoch: 10/20 [2140/4535 (80%)]\tLoss: 0.485748\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 10/20 [4535/4535 (80%)]\tLoss: 0.485748\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 11/20 [0/4535 (0%)]\tLoss: 0.488589\n",
      " Train Epoch: 11/20 [1000/4535 (20%)]\tLoss: 0.466552\n",
      " Train Epoch: 11/20 [2000/4535 (40%)]\tLoss: 0.464230\n",
      " Train Epoch: 11/20 [3000/4535 (60%)]\tLoss: 0.479442\n",
      " Train Epoch: 11/20 [2140/4535 (80%)]\tLoss: 0.482269\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 11/20 [4535/4535 (80%)]\tLoss: 0.482269\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 12/20 [0/4535 (0%)]\tLoss: 0.479934\n",
      " Train Epoch: 12/20 [1000/4535 (20%)]\tLoss: 0.466323\n",
      " Train Epoch: 12/20 [2000/4535 (40%)]\tLoss: 0.469454\n",
      " Train Epoch: 12/20 [3000/4535 (60%)]\tLoss: 0.474616\n",
      " Train Epoch: 12/20 [2140/4535 (80%)]\tLoss: 0.462599\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 12/20 [4535/4535 (80%)]\tLoss: 0.462599\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 13/20 [0/4535 (0%)]\tLoss: 0.472086\n",
      " Train Epoch: 13/20 [1000/4535 (20%)]\tLoss: 0.469686\n",
      " Train Epoch: 13/20 [2000/4535 (40%)]\tLoss: 0.464468\n",
      " Train Epoch: 13/20 [3000/4535 (60%)]\tLoss: 0.470177\n",
      " Train Epoch: 13/20 [2140/4535 (80%)]\tLoss: 0.458375\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 13/20 [4535/4535 (80%)]\tLoss: 0.458375\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 14/20 [0/4535 (0%)]\tLoss: 0.463163\n",
      " Train Epoch: 14/20 [1000/4535 (20%)]\tLoss: 0.476940\n",
      " Train Epoch: 14/20 [2000/4535 (40%)]\tLoss: 0.462741\n",
      " Train Epoch: 14/20 [3000/4535 (60%)]\tLoss: 0.462642\n",
      " Train Epoch: 14/20 [2140/4535 (80%)]\tLoss: 0.453368\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 14/20 [4535/4535 (80%)]\tLoss: 0.453368\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 15/20 [0/4535 (0%)]\tLoss: 0.452479\n",
      " Train Epoch: 15/20 [1000/4535 (20%)]\tLoss: 0.462829\n",
      " Train Epoch: 15/20 [2000/4535 (40%)]\tLoss: 0.452626\n",
      " Train Epoch: 15/20 [3000/4535 (60%)]\tLoss: 0.473643\n",
      " Train Epoch: 15/20 [2140/4535 (80%)]\tLoss: 0.476208\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 15/20 [4535/4535 (80%)]\tLoss: 0.476208\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 16/20 [0/4535 (0%)]\tLoss: 0.448014\n",
      " Train Epoch: 16/20 [1000/4535 (20%)]\tLoss: 0.467814\n",
      " Train Epoch: 16/20 [2000/4535 (40%)]\tLoss: 0.466868\n",
      " Train Epoch: 16/20 [3000/4535 (60%)]\tLoss: 0.457668\n",
      " Train Epoch: 16/20 [2140/4535 (80%)]\tLoss: 0.459074\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 16/20 [4535/4535 (80%)]\tLoss: 0.459074\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 17/20 [0/4535 (0%)]\tLoss: 0.449947\n",
      " Train Epoch: 17/20 [1000/4535 (20%)]\tLoss: 0.453371\n",
      " Train Epoch: 17/20 [2000/4535 (40%)]\tLoss: 0.457565\n",
      " Train Epoch: 17/20 [3000/4535 (60%)]\tLoss: 0.469037\n",
      " Train Epoch: 17/20 [2140/4535 (80%)]\tLoss: 0.461620\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 17/20 [4535/4535 (80%)]\tLoss: 0.461620\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 18/20 [0/4535 (0%)]\tLoss: 0.450427\n",
      " Train Epoch: 18/20 [1000/4535 (20%)]\tLoss: 0.460091\n",
      " Train Epoch: 18/20 [2000/4535 (40%)]\tLoss: 0.463006\n",
      " Train Epoch: 18/20 [3000/4535 (60%)]\tLoss: 0.458726\n",
      " Train Epoch: 18/20 [2140/4535 (80%)]\tLoss: 0.442097\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 18/20 [4535/4535 (80%)]\tLoss: 0.442097\t Validation Accuracy: 87.59%\n",
      " Train Epoch: 19/20 [0/4535 (0%)]\tLoss: 0.446536\n",
      " Train Epoch: 19/20 [1000/4535 (20%)]\tLoss: 0.453764\n",
      " Train Epoch: 19/20 [2000/4535 (40%)]\tLoss: 0.454557\n",
      " Train Epoch: 19/20 [3000/4535 (60%)]\tLoss: 0.451493\n",
      " Train Epoch: 19/20 [2140/4535 (80%)]\tLoss: 0.477160\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 19/20 [4535/4535 (80%)]\tLoss: 0.477160\t Validation Accuracy: 87.61%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Train Epoch: 20/20 [0/4535 (0%)]\tLoss: 0.442145\n",
      " Train Epoch: 20/20 [1000/4535 (20%)]\tLoss: 0.451385\n",
      " Train Epoch: 20/20 [2000/4535 (40%)]\tLoss: 0.462502\n",
      " Train Epoch: 20/20 [3000/4535 (60%)]\tLoss: 0.452288\n",
      " Train Epoch: 20/20 [2140/4535 (80%)]\tLoss: 0.461297\n",
      "Predictions: tensor([1, 0, 1,  ..., 0, 0, 1])\n",
      "Actual values: tensor([1., 0., 1.,  ..., 0., 0., 1.])\n",
      " Train Epoch: 20/20 [4535/4535 (80%)]\tLoss: 0.461297\t Validation Accuracy: 87.61%\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "#         if cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data) \n",
    "        \n",
    "        # Calculate and appy loss\n",
    "        loss = F.cross_entropy(y_pred, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch+1,\n",
    "            EPOCHS,\n",
    "            batch_idx * len(data), \n",
    "            len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), \n",
    "            loss.cpu().data.item()))\n",
    "    \n",
    "    # NOTE: The Variable class is a wrapper class around a tensor with the added functionalities of back propagation\n",
    "    # For x, take all cols of datafram except the last one\n",
    "    evaluate_x = Variable(valid_loader.dataset.data[:, 0:-1].type_as(torch.FloatTensor()))\n",
    "    # For y, take last col\n",
    "    evaluate_y = Variable(valid_loader.dataset.data[:, -1:])\n",
    "#     if cuda:\n",
    "#         evaluate_x, evaluate_y = evaluate_x.cuda(), evaluate_y.cuda()\n",
    "    model.eval()\n",
    "    output = model(evaluate_x)\n",
    "    pred = output.data.max(1)[1]\n",
    "    \n",
    "    y_labels = torch.flatten(evaluate_y.data)\n",
    "    print('Predictions:', pred)\n",
    "    print('Actual values:', y_labels)\n",
    "    d = pred.eq(y_labels).cpu()\n",
    "    accuracy = d.sum().item()*1./d.size()[0]\n",
    "    \n",
    "    print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Validation Accuracy: {:.2f}%'.format(\n",
    "        epoch+1,\n",
    "        EPOCHS,\n",
    "        len(train_loader.dataset), \n",
    "        len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), \n",
    "        loss.cpu().data.item(),\n",
    "        accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "* Take testing dataset, run it against current model\n",
    "* Evaluate testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Testing Accuracy: 89.88%\n"
     ]
    }
   ],
   "source": [
    "# Follow same steps as when we test with validation set, except with the test set\n",
    "evaluate_x = Variable(test_loader.dataset.data[:, 0:-1].type_as(torch.FloatTensor()))\n",
    "evaluate_y = Variable(test_loader.dataset.data[:, -1:])\n",
    "\n",
    "model.eval()\n",
    "output = model(evaluate_x)\n",
    "pred = output.data.max(1)[1]\n",
    "\n",
    "y_labels = torch.flatten(evaluate_y.data)\n",
    "d = pred.eq(y_labels).cpu()\n",
    "accuracy = d.sum().item()*1./d.size()[0]\n",
    "\n",
    "print('\\n Testing Accuracy: {:.2f}%'.format(\n",
    "\n",
    "    accuracy*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
