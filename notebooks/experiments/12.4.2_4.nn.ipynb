{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook:\n",
    "\n",
    "* on feature set 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install numpy \n",
    "#!pip install pandas\n",
    "#!pip install torch\n",
    "#!pip install sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating data\n",
    "* Import csv we want\n",
    "* Map our features to usuable integers\n",
    "* Map our labels to a one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       aapos codonpos  Ancestral_allele  Polyphen2_HVAR_pred  \\\n",
      "chr pos       ref alt                                                          \n",
      "3   101244651 A   T      560        3                 2                    0   \n",
      "    101244766 G   C      522        2                 2                    0   \n",
      "    101269574 C   A       -1        0                 2                    0   \n",
      "    101275694 G   C      212        2                 2                    0   \n",
      "    101291499 A   C      171        3                 2                    0   \n",
      "\n",
      "                          GenoCanyon_score HUVEC_fitCons_score  \\\n",
      "chr pos       ref alt                                            \n",
      "3   101244651 A   T      0.995787957382137            0.567892   \n",
      "    101244766 G   C      0.997773309808223            0.567892   \n",
      "    101269574 C   A      0.998404006175971            0.057018   \n",
      "    101275694 G   C    1.97468820369911E-4            0.620846   \n",
      "    101291499 A   C      0.985239277427167            0.564101   \n",
      "\n",
      "                          ExAC_cnv.score  SORVA_LOF_MAF0.005_HetOrHom  \\\n",
      "chr pos       ref alt                                                   \n",
      "3   101244651 A   T    0.456470620298397                     0.000399   \n",
      "    101244766 G   C    0.456470620298397                     0.000399   \n",
      "    101269574 C   A    0.456470620298397                     0.000399   \n",
      "    101275694 G   C    0.456470620298397                     0.000399   \n",
      "    101291499 A   C    0.456470620298397                     0.000399   \n",
      "\n",
      "                       Essential_gene_CRISPR  Mutation<1kb  Mutation<5kb  \\\n",
      "chr pos       ref alt                                                      \n",
      "3   101244651 A   T                        1             5             8   \n",
      "    101244766 G   C                        1             5             8   \n",
      "    101269574 C   A                        1             1             1   \n",
      "    101275694 G   C                        1             1             1   \n",
      "    101291499 A   C                        1             1             1   \n",
      "\n",
      "                       Mutation<10kb  Mutation<30kb  Mutation<100kb  \\\n",
      "chr pos       ref alt                                                 \n",
      "3   101244651 A   T                8             14              21   \n",
      "    101244766 G   C                8             14              21   \n",
      "    101269574 C   A                2             11              21   \n",
      "    101275694 G   C                2              5              21   \n",
      "    101291499 A   C                1              8              21   \n",
      "\n",
      "                       Problematic  \n",
      "chr pos       ref alt               \n",
      "3   101244651 A   T              1  \n",
      "    101244766 G   C              1  \n",
      "    101269574 C   A              1  \n",
      "    101275694 G   C              1  \n",
      "    101291499 A   C              1  \n",
      "(7773, 15) (3455, 15)\n"
     ]
    }
   ],
   "source": [
    "#################################################### \n",
    "df = pd.read_pickle(\"./pickled/chr2-4_featureset4.pkl\")\n",
    "df = df.replace({'-': 0})\n",
    "\n",
    "# problematic isn't first:\n",
    "\n",
    "#print(\"LENGTH\",len(df.columns))\n",
    "col = df.pop(\"Problematic\")\n",
    "df[col.name] = col\n",
    "\n",
    "# Split our data into three separate sets for training purposes\n",
    "train, test = train_test_split(df, shuffle=False)\n",
    "train, valid = train_test_split(train, shuffle=False)\n",
    "\n",
    "print(test.head())\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "* Implement the Dataset class supplied by PyTorch\n",
    "  * Used Map style\n",
    "  * API: https://pytorch.org/docs/stable/data.html#map-style-datasets\n",
    "* Instanciate DataLoader using Dataset instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetdbNSFP(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data.values.astype('float'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "            target = self.data[index][-1]\n",
    "            data_val = self.data[index] [:-1]\n",
    "            return data_val,target\n",
    "        \n",
    "train_dataset = DataSetdbNSFP(train)\n",
    "valid_dataset = DataSetdbNSFP(valid)  \n",
    "test_dataset = DataSetdbNSFP(test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=59, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=59, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=59, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "* This model is a simple implementation\n",
    "* Based on https://github.com/ieee8023/NeuralNetwork-Examples/blob/master/pytorch/pytorch-mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # In: nb of features, out: nb of examples\n",
    "        self.fc = nn.Linear(14, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 1000)\n",
    "         # In: nb of examples, out: nb of predictions\n",
    "        self.fc5 = nn.Linear(1000, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 14))\n",
    "        h = F.relu(self.fc(x))\n",
    "        h = self.fc2(h)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.fc3(h)\n",
    "        h = F.relu(self.fc3(h))\n",
    "        h = self.fc4(h)\n",
    "        h = F.relu(self.fc4(h))\n",
    "        h = self.fc5(h)\n",
    "        # Softmax to get the actual labels\n",
    "        return F.softmax(h, dim=1)  ## Note: can potentially use this to get confidence of category predicted \n",
    "    \n",
    "model = Model()\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "\n",
    "# Optimizer based on Adam algorithm, uses a slightly lower rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "* Run a fixed number of epochs\n",
    "* After each epoch, run the validation set to adjust learning\n",
    "* Print results\n",
    "* Based on example: https://github.com/ieee8023/NeuralNetwork-Examples/blob/master/pytorch/pytorch-mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [0/7773 (0%)]\tLoss: 0.784956\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [59/7773 (1%)]\tLoss: 0.649808\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [118/7773 (2%)]\tLoss: 0.642431\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [177/7773 (2%)]\tLoss: 0.638851\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [236/7773 (3%)]\tLoss: 0.567639\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [295/7773 (4%)]\tLoss: 0.614259\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [354/7773 (5%)]\tLoss: 0.647711\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [413/7773 (5%)]\tLoss: 0.559787\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [472/7773 (6%)]\tLoss: 0.566538\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [531/7773 (7%)]\tLoss: 0.564815\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [590/7773 (8%)]\tLoss: 0.640363\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [649/7773 (8%)]\tLoss: 0.653800\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [708/7773 (9%)]\tLoss: 0.637221\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [767/7773 (10%)]\tLoss: 0.521067\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [826/7773 (11%)]\tLoss: 0.522066\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [885/7773 (11%)]\tLoss: 0.620118\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [944/7773 (12%)]\tLoss: 0.698689\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1003/7773 (13%)]\tLoss: 0.660096\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1062/7773 (14%)]\tLoss: 0.665273\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1121/7773 (14%)]\tLoss: 0.600783\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1180/7773 (15%)]\tLoss: 0.533095\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1239/7773 (16%)]\tLoss: 0.586020\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1298/7773 (17%)]\tLoss: 0.703001\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1357/7773 (17%)]\tLoss: 0.600855\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1416/7773 (18%)]\tLoss: 0.617146\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1475/7773 (19%)]\tLoss: 0.634746\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1534/7773 (20%)]\tLoss: 0.618654\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1593/7773 (20%)]\tLoss: 0.619092\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1652/7773 (21%)]\tLoss: 0.699961\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1711/7773 (22%)]\tLoss: 0.621907\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1770/7773 (23%)]\tLoss: 0.514496\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1829/7773 (23%)]\tLoss: 0.620484\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1888/7773 (24%)]\tLoss: 0.584440\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1947/7773 (25%)]\tLoss: 0.551449\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2006/7773 (26%)]\tLoss: 0.567235\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2065/7773 (27%)]\tLoss: 0.597021\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2124/7773 (27%)]\tLoss: 0.601374\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2183/7773 (28%)]\tLoss: 0.618954\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2242/7773 (29%)]\tLoss: 0.599176\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2301/7773 (30%)]\tLoss: 0.634588\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2360/7773 (30%)]\tLoss: 0.550570\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2419/7773 (31%)]\tLoss: 0.635196\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2478/7773 (32%)]\tLoss: 0.632743\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2537/7773 (33%)]\tLoss: 0.597783\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2596/7773 (33%)]\tLoss: 0.550684\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2655/7773 (34%)]\tLoss: 0.618713\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2714/7773 (35%)]\tLoss: 0.568483\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2773/7773 (36%)]\tLoss: 0.554467\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2832/7773 (36%)]\tLoss: 0.687169\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2891/7773 (37%)]\tLoss: 0.529272\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2950/7773 (38%)]\tLoss: 0.539758\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3009/7773 (39%)]\tLoss: 0.640773\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3068/7773 (39%)]\tLoss: 0.618586\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3127/7773 (40%)]\tLoss: 0.705842\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3186/7773 (41%)]\tLoss: 0.708778\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3245/7773 (42%)]\tLoss: 0.649624\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3304/7773 (42%)]\tLoss: 0.623230\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3363/7773 (43%)]\tLoss: 0.627757\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3422/7773 (44%)]\tLoss: 0.654429\n",
      "torch.Size([59, 14])\n",
      "torch.Size([59])\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        print(data.shape)\n",
    "        print(target.shape)\n",
    "        \n",
    "#         if cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data) \n",
    "        \n",
    "        # Calculate and appy loss\n",
    "        loss = F.cross_entropy(y_pred, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch+1,\n",
    "            EPOCHS,\n",
    "            batch_idx * len(data), \n",
    "            len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), \n",
    "            loss.cpu().data.item()))\n",
    "    \n",
    "    # NOTE: The Variable class is a wrapper class around a tensor with the added functionalities of back propagation\n",
    "    # For x, take all cols of datafram except the last one\n",
    "    evaluate_x = Variable(valid_loader.dataset.data[:, 0:-1].type_as(torch.FloatTensor()))\n",
    "    print(evaluate_x)\n",
    "    # For y, take last col\n",
    "    evaluate_y = Variable(valid_loader.dataset.data[:, -1:])\n",
    "#     if cuda:\n",
    "#         evaluate_x, evaluate_y = evaluate_x.cuda(), evaluate_y.cuda()\n",
    "    model.eval()\n",
    "    output = model(evaluate_x)\n",
    "    pred = output.data.max(1)[1]\n",
    "    \n",
    "    y_labels = torch.flatten(evaluate_y.data)\n",
    "    print('Predictions:', pred)\n",
    "    print('Actual values:', y_labels)\n",
    "    d = pred.eq(y_labels).cpu()\n",
    "    accuracy = d.sum().item()*1./d.size()[0]\n",
    "    \n",
    "    print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Validation Accuracy: {:.2f}%'.format(\n",
    "        epoch+1,\n",
    "        EPOCHS,\n",
    "        len(train_loader.dataset), \n",
    "        len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), \n",
    "        loss.cpu().data.item(),\n",
    "        accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "* Take testing dataset, run it against current model\n",
    "* Evaluate testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow same steps as when we test with validation set, except with the test set\n",
    "evaluate_x = Variable(test_loader.dataset.data[:, 0:-1].type_as(torch.FloatTensor()))\n",
    "evaluate_y = Variable(test_loader.dataset.data[:, -1:])\n",
    "\n",
    "model.eval()\n",
    "output = model(evaluate_x)\n",
    "pred = output.data.max(1)[1]\n",
    "\n",
    "\n",
    "y_labels = torch.flatten(evaluate_y.data)\n",
    "d = pred.eq(y_labels).cpu()\n",
    "accuracy = d.sum().item()*1./d.size()[0]  ## Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "print('\\n Testing Accuracy: {:.2f}%'.format(\n",
    "\n",
    "    accuracy*100))\n",
    "\n",
    "print(len(pred))\n",
    "print(len(y_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "* Use sklearn builtins to calculate different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_labels, pred))\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_labels, pred))\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_labels, pred, average=None)\n",
    "print(\"Recall TP/(TP+FN):\", recall)\n",
    "\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_labels, pred, average=None)\n",
    "print(\"Precision TP/(TP+FP):\",precision)\n",
    "\n",
    "\n",
    "# F1 score\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1:\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recalculating accuracy: TP + TN / (all exmaples)\n",
    "print(\"accuracy\", 2362/(1080+2362))\n",
    "1080+2362\n",
    "\n",
    "#Test shape, train shape\n",
    "#(10325, 9) (3442, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
