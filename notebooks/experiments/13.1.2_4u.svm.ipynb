{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# feature set 1 chromosomes 2, 3, 4 + unknown\n",
    "\n",
    "## In this notebook:\n",
    "\n",
    "* single input dataframe which includes unknown labels \n",
    "* using feature set 2\n",
    "* model: SVM (rbf) \n",
    "* only training/test split, no epochs \n",
    "* aucuracy assessement of SVM:  0.39 wow thats bad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torch\n",
    "#!pip install pandas\n",
    "#!pip install numpy\n",
    "#!pip install sklearn\n",
    "#!pip install matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(40905, 9)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# using sklearn goodies\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.datasets import load_wine\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "## pull in pickled dataframe:\n",
    "df = pd.read_pickle(\"./pickled/chr2-4_unk_featureset1.pkl\")\n",
    "df.shape # 40905, 9\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data subset\n",
      "\n",
      "                       aapos TSL codonpos HUVEC_fitCons_score  \\\n",
      "chr pos       ref alt                                           \n",
      "2   25244258  C   T      583   1        2            0.714379   \n",
      "    178586706 C   T    19758   5        1            0.564101   \n",
      "    47403295  G   A       35   1        2             0.56214   \n",
      "\n",
      "                           ExAC_cnv.score          GDI LoFtool_score  \\\n",
      "chr pos       ref alt                                                  \n",
      "2   25244258  C   T     -1.30301309757823     41.83604      1.50E-01   \n",
      "    178586706 C   T                     0  74772.86558      9.71E-01   \n",
      "    47403295  G   A    -0.391627327762734    376.88542      9.71E-03   \n",
      "\n",
      "                       SORVA_LOF_MAF0.005_HetOrHom  Problematic  \n",
      "chr pos       ref alt                                            \n",
      "2   25244258  C   T                       0.001198            1  \n",
      "    178586706 C   T                       0.017173            0  \n",
      "    47403295  G   A                       0.000399            0  \n",
      "\n",
      "Test data subset\n",
      "\n",
      "                     aapos TSL codonpos HUVEC_fitCons_score  \\\n",
      "chr pos     ref alt                                           \n",
      "2   1456232 G   T      257   1        1            0.564101   \n",
      "    1484596 A   T      447   1        1            0.613276   \n",
      "    1484614 T   G      453   1        1            0.613276   \n",
      "\n",
      "                        ExAC_cnv.score         GDI LoFtool_score  \\\n",
      "chr pos     ref alt                                                \n",
      "2   1456232 G   T    -2.47162789907313  2837.88244      8.54E-01   \n",
      "    1484596 A   T    -2.47162789907313  2837.88244      8.54E-01   \n",
      "    1484614 T   G    -2.47162789907313  2837.88244      8.54E-01   \n",
      "\n",
      "                     SORVA_LOF_MAF0.005_HetOrHom  Problematic  \n",
      "chr pos     ref alt                                            \n",
      "2   1456232 G   T                       0.004792            2  \n",
      "    1484596 A   T                       0.004792            1  \n",
      "    1484614 T   G                       0.004792            1  \n"
     ]
    }
   ],
   "source": [
    "## Split into train/test\n",
    "data_copy = df.copy()\n",
    "df_train = data_copy.sample(frac=0.75, random_state=1)\n",
    "df_test = data_copy.drop(df_train.index)\n",
    "\n",
    "print ('Training data subset\\n')\n",
    "print (df_train.head(3))  # head default n=5, first 3 is enough\n",
    "print ('\\nTest data subset\\n')\n",
    "print (df_test.head(3))\n",
    "\n",
    "\n",
    "## Get label:\n",
    "train_labels = df_train.pop('Problematic')  # replace with clinvar when column is available\n",
    "test_labels = df_test.pop('Problematic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done making classifier\n"
     ]
    }
   ],
   "source": [
    "classifier1 = SVC(kernel='rbf', gamma='scale',random_state=42, class_weight=\"balanced\") # default for kernel and gamma. AUC = 0.94\n",
    "#classifier2 = SVC(kernel='linear', gamma=10 ,random_state=42) # 0.94\n",
    "#classifier3 = SVC(kernel='sigmoid', gamma=10 ,random_state=42) # 0.82\n",
    "print(\"done making classifier\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "doen fitting\n"
     ]
    }
   ],
   "source": [
    "classifier1.fit(df_train, train_labels)\n",
    "print(\"done fitting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "y true [2 1 1 ... 0 0 0]\n",
      "y predicted [1 1 1 ... 2 2 1]\n",
      "Confusion matrix: \n",
      " [[1758 4035  957]\n",
      " [ 191 2022  207]\n",
      " [ 237  506  196]]\n",
      "Accuracy: 0.393312889504402\n",
      "Recall TP/(TP+FN): [0.26044444 0.83553719 0.20873269]\n",
      "Precision TP/(TP+FP): [0.8042086  0.30809081 0.14411765]\n",
      "F1: [0.39346464 0.45018368 0.17050892]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Run some stats to see if the model is doing well. \n",
    "y_true = test_labels.to_numpy()\n",
    "y_pred = classifier1.predict(df_test)\n",
    "\n",
    "\n",
    "print(\"y true\", y_true) ## These are both numpy arrays now, why are the metrics returning arrays rather than single f\n",
    "print(\"y predicted\", y_pred)\n",
    "\n",
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_true, y_pred))\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_true, y_pred))\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_true, y_pred, average=None)\n",
    "print(\"Recall TP/(TP+FN):\", recall)\n",
    "\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_true, y_pred, average=None)\n",
    "print(\"Precision TP/(TP+FP):\",precision)\n",
    "\n",
    "# F1 score\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1:\", F1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
