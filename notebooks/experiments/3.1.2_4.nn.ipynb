{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## In this notebook:\n",
    "\n",
    "* 3 input dataframe: chr18, 19, 20\n",
    "* using all numeric scores for predictors\n",
    "* fully connected neural net\n",
    "* training with dataloading and epochs\n",
    "* aucuracy assessement of .954 on held-out test "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numpy in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.16.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.0.3)\n",
      "Requirement already satisfied: python-dateutil>=2.6.1 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2.8.1)\n",
      "Requirement already satisfied: numpy>=1.13.3 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (1.16.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from pandas) (2019.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\vincent\\appdata\\roaming\\python\\python37\\site-packages (from python-dateutil>=2.6.1->pandas) (1.11.0)\n",
      "Requirement already satisfied: torch in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (1.4.0)\n",
      "Requirement already satisfied: sklearn in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (0.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from sklearn) (0.22.2.post1)\n",
      "Requirement already satisfied: numpy>=1.11.0 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.16.2)\n",
      "Requirement already satisfied: joblib>=0.11 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (0.14.1)\n",
      "Requirement already satisfied: scipy>=0.17.0 in c:\\users\\vincent\\appdata\\local\\programs\\python\\python37\\lib\\site-packages (from scikit-learn->sklearn) (1.2.1)\n"
     ]
    }
   ],
   "source": [
    "# !pip install numpy \n",
    "# !pip install pandas\n",
    "# !pip install torch\n",
    "# !pip install sklearn\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn import functional as F\n",
    "from sklearn.metrics import accuracy_score\n",
    "from torch.autograd import Variable\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Manipulating data\n",
    "* Import csv we want\n",
    "* Map our features to usuable integers\n",
    "* Map our labels to a one hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       aapos TSL codonpos HUVEC_fitCons_score  \\\n",
      "chr pos       ref alt                                           \n",
      "3   100748418 A   C      364   3        1            0.714379   \n",
      "    101228876 C   A     1212   1        1            0.564101   \n",
      "    101229541 T   A     1158   1        1            0.638833   \n",
      "    101229574 G   A     1147   1        1            0.638833   \n",
      "    101231117 G   A     1088   1        1            0.613276   \n",
      "\n",
      "                          ExAC_cnv.score        GDI LoFtool_score  \\\n",
      "chr pos       ref alt                                               \n",
      "3   100748418 A   C                    0    55.5296      3.91E-01   \n",
      "    101228876 C   A    0.456470620298397  209.43193      8.64E-01   \n",
      "    101229541 T   A    0.456470620298397  209.43193      8.64E-01   \n",
      "    101229574 G   A    0.456470620298397  209.43193      8.64E-01   \n",
      "    101231117 G   A    0.456470620298397  209.43193      8.64E-01   \n",
      "\n",
      "                       SORVA_LOF_MAF0.005_HetOrHom  Problematic  \n",
      "chr pos       ref alt                                            \n",
      "3   100748418 A   C                       0.000000            0  \n",
      "    101228876 C   A                       0.000399            1  \n",
      "    101229541 T   A                       0.000399            1  \n",
      "    101229574 G   A                       0.000399            0  \n",
      "    101231117 G   A                       0.000399            1  \n",
      "(7743, 9) (3442, 9)\n"
     ]
    }
   ],
   "source": [
    "#################################################### \n",
    "df = pd.read_pickle(\"./pickled/chr2-4_featureset1.pkl\")\n",
    "df = df.replace({'-': 0})\n",
    "\n",
    "# Split our data into two separate sets for training purposes\n",
    "train, test = train_test_split(df, shuffle=False)\n",
    "train, valid = train_test_split(train, shuffle=False)\n",
    "\n",
    "print(test.head())\n",
    "print(train.shape, test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DataLoader\n",
    "* Implement the Dataset class supplied by PyTorch\n",
    "  * Used Map style\n",
    "  * API: https://pytorch.org/docs/stable/data.html#map-style-datasets\n",
    "* Instanciate DataLoader using Dataset instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSetdbNSFP(Dataset):\n",
    "    def __init__(self, data):\n",
    "        self.data = torch.FloatTensor(data.values.astype('float'))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    def __getitem__(self, index):\n",
    "            target = self.data[index][-1]\n",
    "            data_val = self.data[index] [:-1]\n",
    "            return data_val,target\n",
    "        \n",
    "train_dataset = DataSetdbNSFP(train)\n",
    "valid_dataset = DataSetdbNSFP(valid)\n",
    "test_dataset = DataSetdbNSFP(test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=59, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=59, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=59, shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "* This model is a simple implementation\n",
    "* Based on https://github.com/ieee8023/NeuralNetwork-Examples/blob/master/pytorch/pytorch-mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Model, self).__init__()\n",
    "        # In: nb of features, out: nb of examples\n",
    "        self.fc = nn.Linear(8, 1000)\n",
    "        self.fc2 = nn.Linear(1000, 1000)\n",
    "        self.fc3 = nn.Linear(1000, 1000)\n",
    "        self.fc4 = nn.Linear(1000, 1000)\n",
    "         # In: nb of examples, out: nb of predictions\n",
    "        self.fc5 = nn.Linear(1000, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view((-1, 8))\n",
    "        h = F.relu(self.fc(x))\n",
    "        h = self.fc2(h)\n",
    "        h = F.relu(self.fc2(h))\n",
    "        h = self.fc3(h)\n",
    "        h = F.relu(self.fc3(h))\n",
    "        h = self.fc4(h)\n",
    "        h = F.relu(self.fc4(h))\n",
    "        h = self.fc5(h)\n",
    "        # Softmax to get the actual labels\n",
    "        return F.softmax(h, dim=1)  ## Note: can potentially use this to get confidence of category predicted \n",
    "    \n",
    "model = Model()\n",
    "# if cuda:\n",
    "#     model.cuda()\n",
    "\n",
    "# Optimizer based on Adam algorithm, uses a slightly lower rate\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training\n",
    "* Run a fixed number of epochs\n",
    "* After each epoch, run the validation set to adjust learning\n",
    "* Print results\n",
    "* Based on example: https://github.com/ieee8023/NeuralNetwork-Examples/blob/master/pytorch/pytorch-mnist.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [0/7743 (0%)]\tLoss: 0.625754\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [59/7743 (1%)]\tLoss: 0.551613\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [118/7743 (2%)]\tLoss: 0.587049\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [177/7743 (2%)]\tLoss: 0.638531\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [236/7743 (3%)]\tLoss: 0.563189\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [295/7743 (4%)]\tLoss: 0.550676\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [354/7743 (5%)]\tLoss: 0.599822\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [413/7743 (5%)]\tLoss: 0.618498\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [472/7743 (6%)]\tLoss: 0.652270\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [531/7743 (7%)]\tLoss: 0.693498\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [590/7743 (8%)]\tLoss: 0.652198\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [649/7743 (8%)]\tLoss: 0.569072\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [708/7743 (9%)]\tLoss: 0.567485\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [767/7743 (10%)]\tLoss: 0.567975\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [826/7743 (11%)]\tLoss: 0.669583\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [885/7743 (11%)]\tLoss: 0.589874\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [944/7743 (12%)]\tLoss: 0.516873\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1003/7743 (13%)]\tLoss: 0.601397\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1062/7743 (14%)]\tLoss: 0.650091\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1121/7743 (14%)]\tLoss: 0.635296\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1180/7743 (15%)]\tLoss: 0.533600\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1239/7743 (16%)]\tLoss: 0.584448\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1298/7743 (17%)]\tLoss: 0.736991\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1357/7743 (17%)]\tLoss: 0.584535\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1416/7743 (18%)]\tLoss: 0.635297\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1475/7743 (19%)]\tLoss: 0.601399\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1534/7743 (20%)]\tLoss: 0.669846\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1593/7743 (20%)]\tLoss: 0.567499\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1652/7743 (21%)]\tLoss: 0.686144\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1711/7743 (22%)]\tLoss: 0.450629\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1770/7743 (23%)]\tLoss: 0.567499\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1829/7743 (23%)]\tLoss: 0.607192\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1888/7743 (24%)]\tLoss: 0.703092\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [1947/7743 (25%)]\tLoss: 0.618296\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2006/7743 (26%)]\tLoss: 0.703093\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2065/7743 (27%)]\tLoss: 0.516676\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2124/7743 (27%)]\tLoss: 0.584494\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2183/7743 (28%)]\tLoss: 0.618346\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2242/7743 (29%)]\tLoss: 0.669196\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2301/7743 (30%)]\tLoss: 0.550587\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2360/7743 (30%)]\tLoss: 0.600973\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2419/7743 (31%)]\tLoss: 0.736989\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2478/7743 (32%)]\tLoss: 0.568906\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2537/7743 (33%)]\tLoss: 0.635505\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2596/7743 (33%)]\tLoss: 0.533390\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2655/7743 (34%)]\tLoss: 0.571398\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2714/7743 (35%)]\tLoss: 0.669194\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2773/7743 (36%)]\tLoss: 0.601857\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2832/7743 (36%)]\tLoss: 0.652255\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2891/7743 (37%)]\tLoss: 0.601408\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [2950/7743 (38%)]\tLoss: 0.584448\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3009/7743 (39%)]\tLoss: 0.686143\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3068/7743 (39%)]\tLoss: 0.635696\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3127/7743 (40%)]\tLoss: 0.618347\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3186/7743 (41%)]\tLoss: 0.635359\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3245/7743 (42%)]\tLoss: 0.533601\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3304/7743 (42%)]\tLoss: 0.533727\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3363/7743 (43%)]\tLoss: 0.652264\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3422/7743 (44%)]\tLoss: 0.583992\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3481/7743 (45%)]\tLoss: 0.550550\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3540/7743 (45%)]\tLoss: 0.500393\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3599/7743 (46%)]\tLoss: 0.550550\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3658/7743 (47%)]\tLoss: 0.618346\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3717/7743 (48%)]\tLoss: 0.635326\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3776/7743 (48%)]\tLoss: 0.431909\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3835/7743 (49%)]\tLoss: 0.753940\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3894/7743 (50%)]\tLoss: 0.618347\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [3953/7743 (51%)]\tLoss: 0.499702\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4012/7743 (52%)]\tLoss: 0.703092\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4071/7743 (52%)]\tLoss: 0.618346\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4130/7743 (53%)]\tLoss: 0.601402\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4189/7743 (54%)]\tLoss: 0.601519\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4248/7743 (55%)]\tLoss: 0.736991\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4307/7743 (55%)]\tLoss: 0.533600\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4366/7743 (56%)]\tLoss: 0.516651\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4425/7743 (57%)]\tLoss: 0.585007\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4484/7743 (58%)]\tLoss: 0.533604\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4543/7743 (58%)]\tLoss: 0.601400\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4602/7743 (59%)]\tLoss: 0.601397\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4661/7743 (60%)]\tLoss: 0.550619\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4720/7743 (61%)]\tLoss: 0.635296\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4779/7743 (61%)]\tLoss: 0.549998\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4838/7743 (62%)]\tLoss: 0.635300\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4897/7743 (63%)]\tLoss: 0.635296\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [4956/7743 (64%)]\tLoss: 0.618348\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [5015/7743 (64%)]\tLoss: 0.647543\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [5074/7743 (65%)]\tLoss: 0.567906\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n",
      " Train Epoch: 1/20 [5133/7743 (66%)]\tLoss: 0.635296\n",
      "torch.Size([59, 8])\n",
      "torch.Size([59])\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 20\n",
    "\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        print(data.shape)\n",
    "        print(target.shape)\n",
    "        \n",
    "#         if cuda:\n",
    "#             data, target = data.cuda(), target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(data) \n",
    "        \n",
    "        # Calculate and appy loss\n",
    "        loss = F.cross_entropy(y_pred, target.long())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "            epoch+1,\n",
    "            EPOCHS,\n",
    "            batch_idx * len(data), \n",
    "            len(train_loader.dataset),\n",
    "            100. * batch_idx / len(train_loader), \n",
    "            loss.cpu().data.item()))\n",
    "    \n",
    "    # NOTE: The Variable class is a wrapper class around a tensor with the added functionalities of back propagation\n",
    "    # For x, take all cols of datafram except the last one\n",
    "    evaluate_x = Variable(valid_loader.dataset.data[:, 0:-1].type_as(torch.FloatTensor()))\n",
    "    print(evaluate_x)\n",
    "    # For y, take last col\n",
    "    evaluate_y = Variable(valid_loader.dataset.data[:, -1:])\n",
    "#     if cuda:\n",
    "#         evaluate_x, evaluate_y = evaluate_x.cuda(), evaluate_y.cuda()\n",
    "    model.eval()\n",
    "    output = model(evaluate_x)\n",
    "    pred = output.data.max(1)[1]\n",
    "    \n",
    "    y_labels = torch.flatten(evaluate_y.data)\n",
    "    print('Predictions:', pred)\n",
    "    print('Actual values:', y_labels)\n",
    "    d = pred.eq(y_labels).cpu()\n",
    "    accuracy = d.sum().item()*1./d.size()[0]\n",
    "    \n",
    "    print('\\r Train Epoch: {}/{} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Validation Accuracy: {:.2f}%'.format(\n",
    "        epoch+1,\n",
    "        EPOCHS,\n",
    "        len(train_loader.dataset), \n",
    "        len(train_loader.dataset),\n",
    "        100. * batch_idx / len(train_loader), \n",
    "        loss.cpu().data.item(),\n",
    "        accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing\n",
    "* Take testing dataset, run it against current model\n",
    "* Evaluate testing accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Follow same steps as when we test with validation set, except with the test set\n",
    "evaluate_x = Variable(test_loader.dataset.data[:, 0:-1].type_as(torch.FloatTensor()))\n",
    "evaluate_y = Variable(test_loader.dataset.data[:, -1:])\n",
    "\n",
    "model.eval()\n",
    "output = model(evaluate_x)\n",
    "pred = output.data.max(1)[1]\n",
    "\n",
    "y_labels = torch.flatten(evaluate_y.data)\n",
    "d = pred.eq(y_labels).cpu()\n",
    "accuracy = d.sum().item()*1./d.size()[0]  ## Accuracy = (TP+TN)/(TP+TN+FP+FN)\n",
    "\n",
    "print('\\n Testing Accuracy: {:.2f}%'.format(\n",
    "\n",
    "    accuracy*100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Metrics\n",
    "* Use sklearn builtins to calculate different metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "print(\"Confusion matrix: \\n\", confusion_matrix(y_labels, pred))\n",
    "\n",
    "# Accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy:\", accuracy_score(y_labels, pred))\n",
    "\n",
    "# Recall\n",
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_labels, pred, average=None)\n",
    "print(\"Recall TP/(TP+FN):\", recall)\n",
    "\n",
    "# Precision\n",
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_labels, pred, average=None)\n",
    "print(\"Precision TP/(TP+FP):\",precision)\n",
    "\n",
    "\n",
    "# F1 score\n",
    "F1 = 2 * (precision * recall) / (precision + recall)\n",
    "print(\"F1:\", F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
