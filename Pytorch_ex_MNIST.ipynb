{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adapted from https://github.com/pytorch/examples/tree/master/mnist_hogwild \n",
    "from __future__ import print_function\n",
    "import os\n",
    "\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F  # functions are defined, such as relu, softmax, dropout \n",
    "import torch.nn as nn  # wherre the layers are defined\n",
    "import torch.multiprocessing as mp\n",
    "from torchvision import datasets, transforms\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64   # help='input batch size for training (default: 64)'\n",
    "test_batch_size = 1000   # help='input batch size for testing (default: 1000)'\n",
    "epochs = 10       # help='number of epochs to train (default: 10)'\n",
    "lr = 0.01         # help='learning rate (default: 0.01)'\n",
    "momentum = 0.5    # help='SGD momentum (default: 0.5)'\n",
    "seed = 1          # help='random seed (default: 1)'\n",
    "log_interval = 100 # help='how many batches to wait before logging training status'\n",
    "num_processes = 2 # help='how many training processes to use (default: 2)'\n",
    "cuda = False      # help='enables CUDA training'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)  ## is in_channel/out_channel the number of nodes? pretty sure yes\n",
    "        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)  \n",
    "        ## from this, the output chanels are 320, but you need to check the shape to know how to call the next layer\n",
    "        self.fc1 = nn.Linear(320, 50)\n",
    "        self.conv2_drop = nn.Dropout2d()\n",
    "        self.fc1_drop = nn.Dropout()  #alt to F.dropout\n",
    "        self.fc2 = nn.Linear(50, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n",
    "        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n",
    "        x = x.view(-1, 320) ## reshape to rearrange the data, MK does not understand quite how to get this value from their shapes\n",
    "        x = F.relu(self.fc1(x))\n",
    "        # x = F.dropout(x, training=self.training)  # possible to use dropout object instead\n",
    "        x = self.fc1_drop(x)  # p default 0.5, inplate default False\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)  #log prob of each of the 10 classes/digits. not compatible with cross-entropy (log_softmax and nll)\n",
    "    \n",
    "    # note that if you want to use cross-entropy, use that instead of the nll below (and remove the above usage of log_softmax, instead forward would just return X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(seed)\n",
    "#use_cuda = cuda and torch.cuda.is_available()\n",
    "device = torch.device(\"cpu\")  # torch.device(\"cuda\" if use_cuda else \"cpu\") # \n",
    "dataloader_kwargs = {}# {'pin_memory': True} if use_cuda else {}  #maybe buggy, not clear if you need this \n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( \n",
    "    datasets.MNIST('./data', train=True, download=False,  # set download to True for first run\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1, **dataloader_kwargs)\n",
    "\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    datasets.MNIST('./data', train=False, download=False, # set download to True for first run\n",
    "                   transform=transforms.Compose([\n",
    "                       transforms.ToTensor(),\n",
    "                       transforms.Normalize((0.1307,), (0.3081,))\n",
    "                   ])),\n",
    "    batch_size=batch_size, shuffle=True, num_workers=1, **dataloader_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net().to(device)\n",
    "optimizer = optim.SGD(model.parameters(), lr=lr, momentum=momentum)  # type of optimization algo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 5, 5])\n"
     ]
    }
   ],
   "source": [
    "model.conv1.weight   ## play with the model's methods and attributes\n",
    "## use tensor.shape is to check torch.Size \n",
    "print(model.conv1.weight.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch,  1\n",
      "20977\tTrain Epoch: 1 [0/60000 (0%)]\tLoss: 0.655943\n",
      "20977\tTrain Epoch: 1 [6400/60000 (11%)]\tLoss: 0.491729\n",
      "20977\tTrain Epoch: 1 [12800/60000 (21%)]\tLoss: 0.263191\n",
      "20977\tTrain Epoch: 1 [19200/60000 (32%)]\tLoss: 0.398660\n",
      "20977\tTrain Epoch: 1 [25600/60000 (43%)]\tLoss: 0.256739\n",
      "20977\tTrain Epoch: 1 [32000/60000 (53%)]\tLoss: 0.206893\n",
      "20977\tTrain Epoch: 1 [38400/60000 (64%)]\tLoss: 0.286191\n",
      "20977\tTrain Epoch: 1 [44800/60000 (75%)]\tLoss: 0.202419\n",
      "20977\tTrain Epoch: 1 [51200/60000 (85%)]\tLoss: 0.247192\n",
      "20977\tTrain Epoch: 1 [57600/60000 (96%)]\tLoss: 0.145374\n",
      "epoch,  2\n",
      "20977\tTrain Epoch: 2 [0/60000 (0%)]\tLoss: 0.227303\n",
      "20977\tTrain Epoch: 2 [6400/60000 (11%)]\tLoss: 0.192070\n",
      "20977\tTrain Epoch: 2 [12800/60000 (21%)]\tLoss: 0.219509\n",
      "20977\tTrain Epoch: 2 [19200/60000 (32%)]\tLoss: 0.293381\n",
      "20977\tTrain Epoch: 2 [25600/60000 (43%)]\tLoss: 0.186252\n",
      "20977\tTrain Epoch: 2 [32000/60000 (53%)]\tLoss: 0.135143\n",
      "20977\tTrain Epoch: 2 [38400/60000 (64%)]\tLoss: 0.245980\n",
      "20977\tTrain Epoch: 2 [44800/60000 (75%)]\tLoss: 0.461322\n",
      "20977\tTrain Epoch: 2 [51200/60000 (85%)]\tLoss: 0.082058\n",
      "20977\tTrain Epoch: 2 [57600/60000 (96%)]\tLoss: 0.267763\n",
      "epoch,  3\n",
      "20977\tTrain Epoch: 3 [0/60000 (0%)]\tLoss: 0.226582\n",
      "20977\tTrain Epoch: 3 [6400/60000 (11%)]\tLoss: 0.243948\n",
      "20977\tTrain Epoch: 3 [12800/60000 (21%)]\tLoss: 0.326261\n",
      "20977\tTrain Epoch: 3 [19200/60000 (32%)]\tLoss: 0.313837\n",
      "20977\tTrain Epoch: 3 [25600/60000 (43%)]\tLoss: 0.266420\n",
      "20977\tTrain Epoch: 3 [32000/60000 (53%)]\tLoss: 0.157278\n",
      "20977\tTrain Epoch: 3 [38400/60000 (64%)]\tLoss: 0.248574\n",
      "20977\tTrain Epoch: 3 [44800/60000 (75%)]\tLoss: 0.494451\n",
      "20977\tTrain Epoch: 3 [51200/60000 (85%)]\tLoss: 0.106988\n",
      "20977\tTrain Epoch: 3 [57600/60000 (96%)]\tLoss: 0.205466\n",
      "epoch,  4\n",
      "20977\tTrain Epoch: 4 [0/60000 (0%)]\tLoss: 0.261750\n",
      "20977\tTrain Epoch: 4 [6400/60000 (11%)]\tLoss: 0.431319\n",
      "20977\tTrain Epoch: 4 [12800/60000 (21%)]\tLoss: 0.288466\n",
      "20977\tTrain Epoch: 4 [19200/60000 (32%)]\tLoss: 0.314443\n",
      "20977\tTrain Epoch: 4 [25600/60000 (43%)]\tLoss: 0.226367\n",
      "20977\tTrain Epoch: 4 [32000/60000 (53%)]\tLoss: 0.222746\n",
      "20977\tTrain Epoch: 4 [38400/60000 (64%)]\tLoss: 0.253218\n",
      "20977\tTrain Epoch: 4 [44800/60000 (75%)]\tLoss: 0.201033\n",
      "20977\tTrain Epoch: 4 [51200/60000 (85%)]\tLoss: 0.165196\n",
      "20977\tTrain Epoch: 4 [57600/60000 (96%)]\tLoss: 0.327804\n",
      "epoch,  5\n",
      "20977\tTrain Epoch: 5 [0/60000 (0%)]\tLoss: 0.242123\n",
      "20977\tTrain Epoch: 5 [6400/60000 (11%)]\tLoss: 0.279221\n",
      "20977\tTrain Epoch: 5 [12800/60000 (21%)]\tLoss: 0.135992\n",
      "20977\tTrain Epoch: 5 [19200/60000 (32%)]\tLoss: 0.123391\n",
      "20977\tTrain Epoch: 5 [25600/60000 (43%)]\tLoss: 0.269259\n",
      "20977\tTrain Epoch: 5 [32000/60000 (53%)]\tLoss: 0.118404\n",
      "20977\tTrain Epoch: 5 [38400/60000 (64%)]\tLoss: 0.197995\n",
      "20977\tTrain Epoch: 5 [44800/60000 (75%)]\tLoss: 0.226959\n",
      "20977\tTrain Epoch: 5 [51200/60000 (85%)]\tLoss: 0.384107\n",
      "20977\tTrain Epoch: 5 [57600/60000 (96%)]\tLoss: 0.073238\n",
      "epoch,  6\n",
      "20977\tTrain Epoch: 6 [0/60000 (0%)]\tLoss: 0.162660\n",
      "20977\tTrain Epoch: 6 [6400/60000 (11%)]\tLoss: 0.116067\n",
      "20977\tTrain Epoch: 6 [12800/60000 (21%)]\tLoss: 0.239418\n",
      "20977\tTrain Epoch: 6 [19200/60000 (32%)]\tLoss: 0.075939\n",
      "20977\tTrain Epoch: 6 [25600/60000 (43%)]\tLoss: 0.128077\n",
      "20977\tTrain Epoch: 6 [32000/60000 (53%)]\tLoss: 0.234143\n",
      "20977\tTrain Epoch: 6 [38400/60000 (64%)]\tLoss: 0.251271\n",
      "20977\tTrain Epoch: 6 [44800/60000 (75%)]\tLoss: 0.316824\n",
      "20977\tTrain Epoch: 6 [51200/60000 (85%)]\tLoss: 0.137509\n",
      "20977\tTrain Epoch: 6 [57600/60000 (96%)]\tLoss: 0.331272\n",
      "epoch,  7\n",
      "20977\tTrain Epoch: 7 [0/60000 (0%)]\tLoss: 0.162440\n",
      "20977\tTrain Epoch: 7 [6400/60000 (11%)]\tLoss: 0.114619\n",
      "20977\tTrain Epoch: 7 [12800/60000 (21%)]\tLoss: 0.164336\n",
      "20977\tTrain Epoch: 7 [19200/60000 (32%)]\tLoss: 0.331571\n",
      "20977\tTrain Epoch: 7 [25600/60000 (43%)]\tLoss: 0.142479\n",
      "20977\tTrain Epoch: 7 [32000/60000 (53%)]\tLoss: 0.320572\n",
      "20977\tTrain Epoch: 7 [38400/60000 (64%)]\tLoss: 0.153332\n",
      "20977\tTrain Epoch: 7 [44800/60000 (75%)]\tLoss: 0.243416\n",
      "20977\tTrain Epoch: 7 [51200/60000 (85%)]\tLoss: 0.156663\n",
      "20977\tTrain Epoch: 7 [57600/60000 (96%)]\tLoss: 0.211908\n",
      "epoch,  8\n",
      "20977\tTrain Epoch: 8 [0/60000 (0%)]\tLoss: 0.095097\n",
      "20977\tTrain Epoch: 8 [6400/60000 (11%)]\tLoss: 0.195082\n",
      "20977\tTrain Epoch: 8 [12800/60000 (21%)]\tLoss: 0.054969\n",
      "20977\tTrain Epoch: 8 [19200/60000 (32%)]\tLoss: 0.193097\n",
      "20977\tTrain Epoch: 8 [25600/60000 (43%)]\tLoss: 0.148018\n",
      "20977\tTrain Epoch: 8 [32000/60000 (53%)]\tLoss: 0.090043\n",
      "20977\tTrain Epoch: 8 [38400/60000 (64%)]\tLoss: 0.236398\n",
      "20977\tTrain Epoch: 8 [44800/60000 (75%)]\tLoss: 0.203215\n",
      "20977\tTrain Epoch: 8 [51200/60000 (85%)]\tLoss: 0.131372\n",
      "20977\tTrain Epoch: 8 [57600/60000 (96%)]\tLoss: 0.168653\n",
      "epoch,  9\n",
      "20977\tTrain Epoch: 9 [0/60000 (0%)]\tLoss: 0.185046\n",
      "20977\tTrain Epoch: 9 [6400/60000 (11%)]\tLoss: 0.145043\n",
      "20977\tTrain Epoch: 9 [12800/60000 (21%)]\tLoss: 0.525765\n",
      "20977\tTrain Epoch: 9 [19200/60000 (32%)]\tLoss: 0.148878\n",
      "20977\tTrain Epoch: 9 [25600/60000 (43%)]\tLoss: 0.205386\n",
      "20977\tTrain Epoch: 9 [32000/60000 (53%)]\tLoss: 0.148876\n",
      "20977\tTrain Epoch: 9 [38400/60000 (64%)]\tLoss: 0.118372\n",
      "20977\tTrain Epoch: 9 [44800/60000 (75%)]\tLoss: 0.318285\n",
      "20977\tTrain Epoch: 9 [51200/60000 (85%)]\tLoss: 0.122630\n",
      "20977\tTrain Epoch: 9 [57600/60000 (96%)]\tLoss: 0.081078\n",
      "epoch,  10\n",
      "20977\tTrain Epoch: 10 [0/60000 (0%)]\tLoss: 0.135771\n",
      "20977\tTrain Epoch: 10 [6400/60000 (11%)]\tLoss: 0.193065\n",
      "20977\tTrain Epoch: 10 [12800/60000 (21%)]\tLoss: 0.064648\n",
      "20977\tTrain Epoch: 10 [19200/60000 (32%)]\tLoss: 0.098314\n",
      "20977\tTrain Epoch: 10 [25600/60000 (43%)]\tLoss: 0.209383\n",
      "20977\tTrain Epoch: 10 [32000/60000 (53%)]\tLoss: 0.264975\n",
      "20977\tTrain Epoch: 10 [38400/60000 (64%)]\tLoss: 0.192168\n",
      "20977\tTrain Epoch: 10 [44800/60000 (75%)]\tLoss: 0.055864\n",
      "20977\tTrain Epoch: 10 [51200/60000 (85%)]\tLoss: 0.102811\n",
      "20977\tTrain Epoch: 10 [57600/60000 (96%)]\tLoss: 0.054207\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"epoch, \", epoch)\n",
    "    model.train()\n",
    "    pid = os.getpid()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        optimizer.zero_grad()  # set the gradients to zero to start\n",
    "        output = model(data.to(device))\n",
    "        \n",
    "        # loss is a tensor that is attached to the graph of computation, this is the tape that joseph talked about\n",
    "        loss = F.nll_loss(output, target.to(device))  #negative log-liklihood loss function (applied to one forward iteration)\n",
    "        # loss *= 0.1  ## Note: this would actually affect the gradient \n",
    "        val = loss.item() * 0.1  ## note that this would store the value, but not change it. Can look at detach for remove. \n",
    "        loss.backward() # calculate the gradients to see what to \"adjust\"\n",
    "        optimizer.step()  # use the gradients to update the weights\n",
    "        \n",
    "        if batch_idx % log_interval == 0:\n",
    "            print('{}\\tTrain Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                pid, epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Test set: Average loss: 0.0457, Accuracy: 9855/10000 (99%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Test\n",
    "model.eval()\n",
    "test_loss = 0\n",
    "correct = 0\n",
    "with torch.no_grad():  ## important not to add these gradients to the loss above (will affect model if you do)\n",
    "    for data, target in test_loader:\n",
    "        output = model(data.to(device))\n",
    "        # print(output)\n",
    "        test_loss += F.nll_loss(output, target.to(device), reduction='sum').item() # sum up batch loss\n",
    "        # print(test_loss)\n",
    "        pred = output.max(1)[1] # get the index of the max log-probability  \n",
    "        correct += pred.eq(target.to(device)).sum().item()  ## this is part of MNIST dataset, ours will be different. \n",
    "\n",
    "test_loss /= len(test_loader.dataset)\n",
    "print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
    "    test_loss, correct, len(test_loader.dataset),\n",
    "    100. * correct / len(test_loader.dataset)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Net(\n",
      "  (conv1): Conv2d(1, 10, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (conv2): Conv2d(10, 20, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (fc1): Linear(in_features=320, out_features=50, bias=True)\n",
      "  (conv2_drop): Dropout2d(p=0.5, inplace=False)\n",
      "  (fc1_drop): Dropout(p=0.5, inplace=False)\n",
      "  (fc2): Linear(in_features=50, out_features=10, bias=True)\n",
      ")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([5, 8, 6, 5, 8, 2, 1, 6, 1, 5, 4, 0, 1, 1, 4, 5])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(model)\n",
    "target\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[torch.Size([10, 1, 5, 5]), torch.Size([10]), torch.Size([20, 10, 5, 5]), torch.Size([20]), torch.Size([50, 320]), torch.Size([50]), torch.Size([10, 50]), torch.Size([10])]\n"
     ]
    }
   ],
   "source": [
    "print([param.shape for param in model.parameters()])  # this is tricky, spend more time understanding the shapes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
